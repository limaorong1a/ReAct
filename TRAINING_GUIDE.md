# ReAct æ¨¡å‹è®­ç»ƒå®Œæ•´æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
2. [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
3. [æ¨¡å‹è®­ç»ƒ](#æ¨¡å‹è®­ç»ƒ)
4. [æ¨¡å‹è¯„ä¼°](#æ¨¡å‹è¯„ä¼°)
5. [æ¨¡å‹éƒ¨ç½²åˆ° Ollama](#æ¨¡å‹éƒ¨ç½²åˆ°-ollama)
6. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### ç¡¬ä»¶è¦æ±‚

| æ¨¡å‹å¤§å° | è®­ç»ƒæ–¹æ³• | æ˜¾å­˜éœ€æ±‚ | æ¨èé…ç½® |
|---------|---------|---------|---------|
| Qwen2.5-0.5B | LoRA | 4-6 GB | GTX 1660 Ti |
| Qwen2.5-3B | LoRA | 8-12 GB | RTX 3060 |
| Qwen2.5-7B | LoRA | 16-20 GB | RTX 4060 Ti 16GB |
| Qwen2.5-7B | QLoRA | 10-14 GB | RTX 3060 12GB |
| Qwen2.5-14B | QLoRA | 18-24 GB | RTX 4090 |

### è½¯ä»¶å®‰è£…

#### æ–¹æ³• 1ï¼šä½¿ç”¨ LLaMA-Factoryï¼ˆæ¨èæ–°æ‰‹ï¼‰

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# 2. å®‰è£…ä¾èµ–
pip install -e .

# 3. å¯åŠ¨ Web UIï¼ˆå¯é€‰ï¼‰
llamafactory-cli webui
# æ‰“å¼€æµè§ˆå™¨è®¿é—® http://localhost:7860
```

#### æ–¹æ³• 2ï¼šä½¿ç”¨ Unslothï¼ˆæ¨èè¿½æ±‚é€Ÿåº¦ï¼‰

```bash
# å®‰è£… Unsloth
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install --no-deps trl peft accelerate bitsandbytes
```

#### æ–¹æ³• 3ï¼šä½¿ç”¨ Axolotlï¼ˆæ¨èé«˜çº§ç”¨æˆ·ï¼‰

```bash
git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl
pip install packaging
pip install -e '.[flash-attn,deepspeed]'
```

---

## ğŸ“Š æ•°æ®å‡†å¤‡

### Step 1: è¿è¡Œæ•°æ®è½¬æ¢è„šæœ¬

```bash
python prepare_training_data.py
```

è¿™å°†ç”Ÿæˆï¼š
- `react_train_alpaca.json` - Alpaca æ ¼å¼è®­ç»ƒé›†
- `react_test_alpaca.json` - Alpaca æ ¼å¼æµ‹è¯•é›†
- `react_train_sharegpt.json` - ShareGPT æ ¼å¼è®­ç»ƒé›†
- `react_test_sharegpt.json` - ShareGPT æ ¼å¼æµ‹è¯•é›†

### Step 2: é…ç½® LLaMA-Factory æ•°æ®é›†

åˆ›å»º `LLaMA-Factory/data/dataset_info.json`ï¼Œæ·»åŠ ï¼š

```json
{
  "react_training": {
    "file_name": "react_train_alpaca.json",
    "columns": {
      "prompt": "instruction",
      "query": "input",
      "response": "output"
    }
  },
  "react_testing": {
    "file_name": "react_test_alpaca.json",
    "columns": {
      "prompt": "instruction",
      "query": "input",
      "response": "output"
    }
  }
}
```

å°†ç”Ÿæˆçš„ JSON æ–‡ä»¶å¤åˆ¶åˆ° `LLaMA-Factory/data/` ç›®å½•ï¼š

```bash
cp react_*.json LLaMA-Factory/data/
```

---

## ğŸš€ æ¨¡å‹è®­ç»ƒ

### æ–¹æ³• Aï¼šä½¿ç”¨ LLaMA-Factory Web UIï¼ˆæœ€ç®€å•ï¼‰

1. å¯åŠ¨ Web UIï¼š
```bash
cd LLaMA-Factory
llamafactory-cli webui
```

2. åœ¨æµè§ˆå™¨ä¸­é…ç½®ï¼š
   - **æ¨¡å‹åç§°**: Qwen/Qwen2.5-7B-Instruct
   - **å¾®è°ƒæ–¹æ³•**: lora
   - **æ•°æ®é›†**: react_training
   - **å­¦ä¹ ç‡**: 5e-5
   - **è®­ç»ƒè½®æ•°**: 3
   - **LoRA rank**: 16

3. ç‚¹å‡»"å¼€å§‹è®­ç»ƒ"

### æ–¹æ³• Bï¼šä½¿ç”¨å‘½ä»¤è¡Œè®­ç»ƒ

```bash
cd LLaMA-Factory

# LoRA è®­ç»ƒ
llamafactory-cli train \
    --stage sft \
    --do_train \
    --model_name_or_path Qwen/Qwen2.5-7B-Instruct \
    --dataset react_training \
    --template qwen \
    --finetuning_type lora \
    --lora_target all \
    --output_dir ./saves/qwen2.5-7b-react \
    --overwrite_cache \
    --overwrite_output_dir \
    --cutoff_len 2048 \
    --preprocessing_num_workers 16 \
    --per_device_train_batch_size 2 \
    --per_device_eval_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --warmup_ratio 0.1 \
    --save_steps 100 \
    --eval_steps 100 \
    --evaluation_strategy steps \
    --load_best_model_at_end \
    --learning_rate 5e-5 \
    --num_train_epochs 3.0 \
    --val_size 0.1 \
    --plot_loss \
    --fp16
```

### æ–¹æ³• Cï¼šä½¿ç”¨ Unslothï¼ˆæœ€å¿«ï¼‰

åˆ›å»º `train_with_unsloth.py`:

```python
from unsloth import FastLanguageModel
import torch
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2.5-7B-Instruct",
    max_seq_length=2048,
    dtype=None,  # è‡ªåŠ¨æ£€æµ‹
    load_in_4bit=True,  # ä½¿ç”¨ 4-bit é‡åŒ–
)

# 2. é…ç½® LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)

# 3. åŠ è½½æ•°æ®
dataset = load_dataset("json", data_files={
    "train": "react_train_alpaca.json",
    "test": "react_test_alpaca.json"
})

# 4. æ ¼å¼åŒ–æ•°æ®
def formatting_func(examples):
    texts = []
    for instruction, input_text, output in zip(
        examples["instruction"],
        examples["input"],
        examples["output"]
    ):
        text = f"### Instruction:\n{instruction}\n\n"
        if input_text:
            text += f"### Input:\n{input_text}\n\n"
        text += f"### Response:\n{output}"
        texts.append(text)
    return {"text": texts}

train_dataset = dataset["train"].map(formatting_func, batched=True)
eval_dataset = dataset["test"].map(formatting_func, batched=True)

# 5. è®­ç»ƒ
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    dataset_text_field="text",
    max_seq_length=2048,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,
        warmup_ratio=0.1,
        num_train_epochs=3,
        learning_rate=5e-5,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="cosine",
        seed=3407,
        output_dir="./output/qwen2.5-7b-react",
        evaluation_strategy="steps",
        eval_steps=100,
        save_steps=100,
    ),
)

trainer.train()

# 6. ä¿å­˜æ¨¡å‹
model.save_pretrained("./output/qwen2.5-7b-react-final")
tokenizer.save_pretrained("./output/qwen2.5-7b-react-final")
```

è¿è¡Œè®­ç»ƒï¼š
```bash
python train_with_unsloth.py
```

---

## ğŸ“ˆ æ¨¡å‹è¯„ä¼°

### ä½¿ç”¨ LLaMA-Factory è¯„ä¼°

```bash
llamafactory-cli train \
    --stage sft \
    --do_eval \
    --model_name_or_path Qwen/Qwen2.5-7B-Instruct \
    --adapter_name_or_path ./saves/qwen2.5-7b-react \
    --dataset react_testing \
    --template qwen \
    --finetuning_type lora \
    --output_dir ./evaluation \
    --per_device_eval_batch_size 2 \
    --predict_with_generate
```

### æ‰‹åŠ¨æµ‹è¯•

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)

# åŠ è½½ LoRA é€‚é…å™¨
model = PeftModel.from_pretrained(base_model, "./saves/qwen2.5-7b-react")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# æµ‹è¯•
test_input = """You are in the middle of a room. Looking quickly around you, you see a cabinet 10, a cabinet 9, a cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a diningtable 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.
Your task is to: heat some potato and put it in diningtable.

Please solve this task using ReAct (Reasoning + Acting) approach."""

inputs = tokenizer(test_input, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=1024)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(response)
```

---

## ğŸ”„ æ¨¡å‹éƒ¨ç½²åˆ° Ollama

### Step 1: åˆå¹¶ LoRA æƒé‡

```bash
cd LLaMA-Factory

# åˆå¹¶ LoRA åˆ°åŸºç¡€æ¨¡å‹
llamafactory-cli export \
    --model_name_or_path Qwen/Qwen2.5-7B-Instruct \
    --adapter_name_or_path ./saves/qwen2.5-7b-react \
    --template qwen \
    --finetuning_type lora \
    --export_dir ./merged_model \
    --export_size 2 \
    --export_device cpu \
    --export_legacy_format False
```

### Step 2: è½¬æ¢ä¸º GGUF æ ¼å¼ï¼ˆOllama éœ€è¦ï¼‰

```bash
# å®‰è£… llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# è½¬æ¢ä¸º GGUF
python convert.py /path/to/merged_model --outfile qwen2.5-7b-react.gguf --outtype f16

# é‡åŒ–ï¼ˆå¯é€‰ï¼Œå‡å°æ¨¡å‹å¤§å°ï¼‰
./quantize qwen2.5-7b-react.gguf qwen2.5-7b-react-q4_k_m.gguf Q4_K_M
```

### Step 3: åˆ›å»º Modelfile

åˆ›å»º `Modelfile`:

```dockerfile
FROM ./qwen2.5-7b-react-q4_k_m.gguf

TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
{{ end }}{{ .Response }}<|im_end|>
"""

PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
PARAMETER temperature 0.7
PARAMETER top_p 0.8
PARAMETER top_k 20

SYSTEM """You are a helpful AI assistant trained to solve tasks using ReAct (Reasoning + Acting) approach. You think step by step, take actions, and observe results."""
```

### Step 4: å¯¼å…¥åˆ° Ollama

```bash
ollama create qwen2.5-react -f Modelfile
```

### Step 5: æµ‹è¯•

```bash
ollama run qwen2.5-react

>>> You are in the middle of a room. Your task is to: heat some egg and put it in diningtable. Please solve this.
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**æ–¹æ¡ˆ 1**: ä½¿ç”¨ QLoRAï¼ˆ4-bit é‡åŒ–ï¼‰
```yaml
quantization_bit: 4  # åœ¨é…ç½®ä¸­æ·»åŠ 
```

**æ–¹æ¡ˆ 2**: å‡å°æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦
```yaml
per_device_train_batch_size: 1
cutoff_len: 1024
```

**æ–¹æ¡ˆ 3**: ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
```yaml
gradient_checkpointing: true
```

**æ–¹æ¡ˆ 4**: é€‰æ‹©æ›´å°çš„æ¨¡å‹ï¼ˆQwen2.5-3B æˆ– 0.5Bï¼‰

### Q2: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢ï¼Ÿ

1. **ä½¿ç”¨ Unsloth**ï¼ˆæé€Ÿ 2-5 å€ï¼‰
2. **ä½¿ç”¨ Flash Attention 2**
```bash
pip install flash-attn --no-build-isolation
```
3. **å‡å°‘åºåˆ—é•¿åº¦**ï¼ˆå¦‚æœç¤ºä¾‹å…è®¸ï¼‰
4. **ä½¿ç”¨å¤š GPU**ï¼ˆDDP/FSDPï¼‰

### Q3: æ¨¡å‹è¿‡æ‹Ÿåˆï¼Ÿ

1. **å¢åŠ  Dropout**
```yaml
lora_dropout: 0.1
```
2. **ä½¿ç”¨æƒé‡è¡°å‡**
```yaml
weight_decay: 0.01
```
3. **å‡å°‘è®­ç»ƒè½®æ•°**
```yaml
num_train_epochs: 2
```
4. **å¢åŠ è®­ç»ƒæ•°æ®**ï¼ˆç”Ÿæˆæ›´å¤šç¤ºä¾‹ï¼‰

### Q4: å¦‚ä½•ç›‘æ§è®­ç»ƒï¼Ÿ

ä½¿ç”¨ TensorBoard:
```bash
tensorboard --logdir ./logs
```

æˆ–ä½¿ç”¨ Weights & Biases:
```bash
pip install wandb
wandb login
# åœ¨é…ç½®ä¸­è®¾ç½® report_to: wandb
```

### Q5: å¦‚ä½•é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡ï¼Ÿ

- **LoRA å¾®è°ƒ**: 1e-4 åˆ° 5e-5
- **QLoRA å¾®è°ƒ**: 1e-4 åˆ° 2e-4
- **å…¨é‡å¾®è°ƒ**: 1e-5 åˆ° 5e-6

å»ºè®®ä½¿ç”¨å­¦ä¹ ç‡é¢„çƒ­ï¼ˆwarmup_ratio: 0.1ï¼‰

---

## ğŸ“š å‚è€ƒèµ„æº

- **LLaMA-Factory**: https://github.com/hiyouga/LLaMA-Factory
- **Unsloth**: https://github.com/unslothai/unsloth
- **Axolotl**: https://github.com/OpenAccess-AI-Collective/axolotl
- **Qwen æ¨¡å‹**: https://huggingface.co/Qwen
- **Ollama**: https://ollama.ai/

---

## ğŸ’¡ è®­ç»ƒå»ºè®®

1. **ä»å°æ¨¡å‹å¼€å§‹**ï¼šå…ˆç”¨ Qwen2.5-3B æµ‹è¯•æµç¨‹
2. **ä½¿ç”¨éªŒè¯é›†**ï¼šç›‘æ§è¿‡æ‹Ÿåˆæƒ…å†µ
3. **ä¿å­˜æ£€æŸ¥ç‚¹**ï¼šå®šæœŸä¿å­˜ï¼Œé˜²æ­¢è®­ç»ƒä¸­æ–­
4. **è®°å½•å®éªŒ**ï¼šä½¿ç”¨ wandb æˆ– tensorboard
5. **æµ‹è¯•ç”Ÿæˆè´¨é‡**ï¼šä¸åªçœ‹ lossï¼Œè¦å®é™…æµ‹è¯•ç”Ÿæˆæ•ˆæœ

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸ‰


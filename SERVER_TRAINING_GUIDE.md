# RTX 5090 æœåŠ¡å™¨ç«¯è®­ç»ƒæŒ‡å—

## ðŸ–¥ï¸ ä½ çš„æœåŠ¡å™¨é…ç½®

```
GPU: NVIDIA GeForce RTX 5090 D Ã— 1 (24GB æ˜¾å­˜)
CPU: 16 vCPU
å†…å­˜: 47GB RAM
å­˜å‚¨: /data_nvme (NVMe SSD)
```

**æ€§èƒ½è¯„ä¼°**: â­â­â­â­â­ é¡¶çº§é…ç½®ï¼
- å¯ä»¥è®­ç»ƒ Qwen2.5-14B ç”šè‡³æ›´å¤§çš„æ¨¡åž‹
- æ”¯æŒè¾ƒå¤§çš„ batch sizeï¼Œè®­ç»ƒé€Ÿåº¦å¿«
- å†…å­˜å’Œå­˜å‚¨å……è¶³ï¼Œæ— éœ€æ‹…å¿ƒ

---

## ðŸ“‹ å®Œæ•´è®­ç»ƒæµç¨‹

### é˜¶æ®µ 1: æœåŠ¡å™¨çŽ¯å¢ƒé…ç½®

#### Step 1: è¿žæŽ¥åˆ°æœåŠ¡å™¨

```bash
# SSH è¿žæŽ¥ï¼ˆæ›¿æ¢ä¸ºä½ çš„æœåŠ¡å™¨åœ°å€å’Œç”¨æˆ·åï¼‰
ssh username@your-server-ip

# æˆ–ä½¿ç”¨å¯†é’¥
ssh -i /path/to/key.pem username@your-server-ip
```

#### Step 2: æ£€æŸ¥ GPU å’Œ CUDA

```bash
# æ£€æŸ¥ GPU
nvidia-smi

# åº”è¯¥çœ‹åˆ° RTX 5090 çš„ä¿¡æ¯
# è®°ä¸‹ CUDA Versionï¼ˆä¾‹å¦‚ 12.2ï¼‰

# æ£€æŸ¥ CUDA è·¯å¾„
echo $CUDA_HOME
nvcc --version
```

#### Step 3: åˆ›å»ºå·¥ä½œç›®å½•

```bash
# ä½¿ç”¨ NVMe æ•°æ®ç›˜ï¼ˆé€Ÿåº¦å¿«ï¼‰
cd /data_nvme

# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir -p react_training
cd react_training

# åˆ›å»ºå­ç›®å½•
mkdir -p {data,models,output,logs,scripts}
```

#### Step 4: é…ç½® Python çŽ¯å¢ƒ

```bash
# æ£€æŸ¥ Python ç‰ˆæœ¬ï¼ˆéœ€è¦ 3.8+ï¼‰
python --version

# åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒ
python -m venv react_env

# æ¿€æ´»çŽ¯å¢ƒ
source react_env/bin/activate

# å‡çº§ pip
pip install --upgrade pip setuptools wheel

# å®‰è£…åŸºç¡€ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
# æ³¨æ„ï¼šæ ¹æ®ä½ çš„ CUDA ç‰ˆæœ¬é€‰æ‹©å¯¹åº”çš„ PyTorch ç‰ˆæœ¬
# cu121 = CUDA 12.1, cu118 = CUDA 11.8

# éªŒè¯ PyTorch å’Œ GPU
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0)}')"
```

---

### é˜¶æ®µ 2: å®‰è£…è®­ç»ƒæ¡†æž¶

#### é€‰é¡¹ A: LLaMA-Factoryï¼ˆæŽ¨èï¼ŒåŠŸèƒ½å…¨é¢ï¼‰

```bash
cd /data_nvme/react_training

# å…‹éš†ä»“åº“
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# å®‰è£…
pip install -e .[torch,metrics]

# å®‰è£…é¢å¤–ä¾èµ–ï¼ˆå¯é€‰ä½†æŽ¨èï¼‰
pip install flash-attn --no-build-isolation  # åŠ é€Ÿè®­ç»ƒ
pip install deepspeed  # æ”¯æŒå¤§æ¨¡åž‹è®­ç»ƒ

# éªŒè¯å®‰è£…
llamafactory-cli version
```

#### é€‰é¡¹ B: Unslothï¼ˆé€Ÿåº¦æœ€å¿«ï¼‰

```bash
# Unsloth å®‰è£…
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install --no-deps trl peft accelerate bitsandbytes

# éªŒè¯
python -c "import unsloth; print('Unsloth installed successfully')"
```

---

### é˜¶æ®µ 3: ä¸Šä¼ æ•°æ®åˆ°æœåŠ¡å™¨

#### æ–¹æ³• 1: ä½¿ç”¨ SCPï¼ˆæŽ¨èï¼‰

```bash
# åœ¨æœ¬åœ°ç”µè„‘è¿è¡Œï¼ˆWindows PowerShell æˆ– Mac/Linux Terminalï¼‰

# é¦–å…ˆåœ¨æœ¬åœ°è¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬
python prepare_training_data.py

# ä¸Šä¼ æ•°æ®æ–‡ä»¶åˆ°æœåŠ¡å™¨
scp react_train_alpaca.json username@server-ip:/data_nvme/react_training/data/
scp react_test_alpaca.json username@server-ip:/data_nvme/react_training/data/
scp react_train_sharegpt.json username@server-ip:/data_nvme/react_training/data/
scp react_test_sharegpt.json username@server-ip:/data_nvme/react_training/data/

# æˆ–æ‰¹é‡ä¸Šä¼ 
scp react_*.json username@server-ip:/data_nvme/react_training/data/
```

#### æ–¹æ³• 2: ä½¿ç”¨ rsyncï¼ˆæ›´é«˜æ•ˆï¼‰

```bash
# åœ¨æœ¬åœ°è¿è¡Œ
rsync -avz --progress react_*.json username@server-ip:/data_nvme/react_training/data/
```

#### æ–¹æ³• 3: ä½¿ç”¨ SFTP æˆ– FTP å®¢æˆ·ç«¯

- **Windows**: ä½¿ç”¨ WinSCP, FileZilla
- **Mac**: ä½¿ç”¨ Cyberduck, FileZilla
- **Linux**: ä½¿ç”¨ FileZilla

#### æ–¹æ³• 4: åœ¨æœåŠ¡å™¨ä¸Šç›´æŽ¥å‡†å¤‡æ•°æ®ï¼ˆå¦‚æžœæœ‰åŽŸå§‹æ•°æ®ï¼‰

```bash
# ä¸Šä¼ åŽŸå§‹ JSON æ–‡ä»¶å¤¹
scp -r generated_samples_* username@server-ip:/data_nvme/react_training/

# ä¸Šä¼ å‡†å¤‡è„šæœ¬
scp prepare_training_data.py username@server-ip:/data_nvme/react_training/

# åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œ
ssh username@server-ip
cd /data_nvme/react_training
source react_env/bin/activate
python prepare_training_data.py
```

---

### é˜¶æ®µ 4: é…ç½®è®­ç»ƒå‚æ•°

#### åˆ›å»ºè®­ç»ƒé…ç½®æ–‡ä»¶

åœ¨æœåŠ¡å™¨ä¸Šåˆ›å»º `/data_nvme/react_training/train_config.yaml`ï¼š

```bash
cat > /data_nvme/react_training/train_config.yaml << 'EOF'
# RTX 5090 ä¼˜åŒ–é…ç½® - Qwen2.5-7B LoRA å¾®è°ƒ

### æ¨¡åž‹é…ç½®
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
# æˆ–ä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼ˆå¦‚æžœå·²ä¸‹è½½ï¼‰
# model_name_or_path: /data_nvme/react_training/models/Qwen2.5-7B-Instruct

### æ•°æ®é…ç½®
dataset: react_training
template: qwen
cutoff_len: 2048
preprocessing_num_workers: 8

### LoRA é…ç½®
finetuning_type: lora
lora_target: all
lora_rank: 32  # RTX 5090 æ˜¾å­˜å……è¶³ï¼Œå¯ä»¥ç”¨æ›´å¤§çš„ rank
lora_alpha: 64
lora_dropout: 0.05
use_rslora: true  # æå‡è®­ç»ƒç¨³å®šæ€§

### è®­ç»ƒå‚æ•°ï¼ˆä¼˜åŒ–ç”¨äºŽ RTX 5090ï¼‰
num_train_epochs: 3
per_device_train_batch_size: 4  # RTX 5090 å¯ä»¥ç”¨æ›´å¤§çš„ batch
per_device_eval_batch_size: 4
gradient_accumulation_steps: 4  # æœ‰æ•ˆ batch = 4*4 = 16
learning_rate: 5.0e-5
lr_scheduler_type: cosine
warmup_ratio: 0.1
max_grad_norm: 1.0

### ä¼˜åŒ–å™¨é…ç½®
optim: adamw_torch_fused  # RTX 5090 æ”¯æŒ fused ä¼˜åŒ–å™¨ï¼Œæ›´å¿«
weight_decay: 0.01

### è¾“å‡ºé…ç½®
output_dir: /data_nvme/react_training/output/qwen2.5-7b-react
logging_dir: /data_nvme/react_training/logs
logging_steps: 10
save_steps: 100
save_total_limit: 3  # åªä¿ç•™æœ€è¿‘ 3 ä¸ªæ£€æŸ¥ç‚¹
eval_steps: 100

### è¯„ä¼°é…ç½®
evaluation_strategy: steps
val_size: 0.1
load_best_model_at_end: true
metric_for_best_model: eval_loss

### æ€§èƒ½ä¼˜åŒ–ï¼ˆRTX 5090 ä¸“å±žï¼‰
bf16: true  # RTX 5090 æ”¯æŒ BF16ï¼Œæ¯” FP16 æ›´ç¨³å®š
# fp16: false  # ä¸ä½¿ç”¨ FP16
gradient_checkpointing: true
ddp_find_unused_parameters: false

# Flash Attention 2ï¼ˆå¦‚æžœå·²å®‰è£…ï¼‰
use_flash_attention_2: true

### æŽ¨ç†é…ç½®
predict_with_generate: true
max_new_tokens: 1024
do_sample: false  # è¯„ä¼°æ—¶ä½¿ç”¨ greedy decoding

### å…¶ä»–é…ç½®
report_to: tensorboard
overwrite_output_dir: true
save_only_model: false
seed: 42
EOF
```

#### é…ç½®æ•°æ®é›†ä¿¡æ¯

```bash
# å¦‚æžœä½¿ç”¨ LLaMA-Factory
cat > /data_nvme/react_training/LLaMA-Factory/data/dataset_info.json << 'EOF'
{
  "react_training": {
    "file_name": "/data_nvme/react_training/data/react_train_alpaca.json",
    "columns": {
      "prompt": "instruction",
      "query": "input",
      "response": "output"
    }
  },
  "react_testing": {
    "file_name": "/data_nvme/react_training/data/react_test_alpaca.json",
    "columns": {
      "prompt": "instruction",
      "query": "input",
      "response": "output"
    }
  }
}
EOF
```

---

### é˜¶æ®µ 5: å¼€å§‹è®­ç»ƒ

#### æ–¹æ³• 1: ä½¿ç”¨ LLaMA-Factory å‘½ä»¤è¡Œ

åˆ›å»ºè®­ç»ƒè„šæœ¬ `/data_nvme/react_training/scripts/train.sh`ï¼š

```bash
cat > /data_nvme/react_training/scripts/train.sh << 'EOF'
#!/bin/bash

# æ¿€æ´»è™šæ‹ŸçŽ¯å¢ƒ
source /data_nvme/react_training/react_env/bin/activate

# è®¾ç½®å·¥ä½œç›®å½•
cd /data_nvme/react_training/LLaMA-Factory

# è®¾ç½®çŽ¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0
export WANDB_DISABLED=true  # å¦‚æžœä¸ä½¿ç”¨ wandb

# å¼€å§‹è®­ç»ƒ
llamafactory-cli train \
    --stage sft \
    --do_train \
    --model_name_or_path Qwen/Qwen2.5-7B-Instruct \
    --dataset react_training \
    --template qwen \
    --finetuning_type lora \
    --lora_target all \
    --lora_rank 32 \
    --lora_alpha 64 \
    --output_dir /data_nvme/react_training/output/qwen2.5-7b-react \
    --overwrite_cache \
    --overwrite_output_dir \
    --cutoff_len 2048 \
    --preprocessing_num_workers 8 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --warmup_ratio 0.1 \
    --save_steps 100 \
    --eval_steps 100 \
    --evaluation_strategy steps \
    --load_best_model_at_end \
    --learning_rate 5e-5 \
    --num_train_epochs 3.0 \
    --val_size 0.1 \
    --plot_loss \
    --bf16 \
    --use_flash_attention_2 \
    --gradient_checkpointing
EOF

# æ·»åŠ æ‰§è¡Œæƒé™
chmod +x /data_nvme/react_training/scripts/train.sh
```

è¿è¡Œè®­ç»ƒï¼š

```bash
# åŽå°è¿è¡Œï¼ˆæŽ¨èï¼‰
nohup /data_nvme/react_training/scripts/train.sh > /data_nvme/react_training/logs/train.log 2>&1 &

# è®°å½•è¿›ç¨‹ ID
echo $! > /data_nvme/react_training/train.pid

# æŸ¥çœ‹å®žæ—¶æ—¥å¿—
tail -f /data_nvme/react_training/logs/train.log
```

#### æ–¹æ³• 2: ä½¿ç”¨ tmux æˆ– screenï¼ˆæŽ¨èï¼‰

```bash
# å®‰è£… tmuxï¼ˆå¦‚æžœæ²¡æœ‰ï¼‰
sudo apt-get install tmux

# åˆ›å»ºæ–°ä¼šè¯
tmux new -s react_training

# åœ¨ tmux ä¸­è¿è¡Œè®­ç»ƒ
source /data_nvme/react_training/react_env/bin/activate
cd /data_nvme/react_training/scripts
./train.sh

# åˆ†ç¦»ä¼šè¯ï¼ˆè®­ç»ƒç»§ç»­è¿è¡Œï¼‰
# æŒ‰ Ctrl+Bï¼Œç„¶åŽæŒ‰ D

# é‡æ–°è¿žæŽ¥
tmux attach -t react_training

# æŸ¥çœ‹æ‰€æœ‰ä¼šè¯
tmux ls
```

---

### é˜¶æ®µ 6: ç›‘æŽ§è®­ç»ƒè¿›åº¦

#### 1. æŸ¥çœ‹å®žæ—¶æ—¥å¿—

```bash
# å®žæ—¶æŸ¥çœ‹æ—¥å¿—
tail -f /data_nvme/react_training/logs/train.log

# æŸ¥çœ‹æœ€è¿‘çš„é”™è¯¯
grep -i error /data_nvme/react_training/logs/train.log
```

#### 2. ç›‘æŽ§ GPU ä½¿ç”¨

```bash
# å®žæ—¶ç›‘æŽ§ GPU
watch -n 1 nvidia-smi

# æˆ–ä½¿ç”¨ gpustatï¼ˆæ›´å‹å¥½ï¼‰
pip install gpustat
gpustat -i 1

# åŽå°ç›‘æŽ§å¹¶è®°å½•
while true; do
    nvidia-smi --query-gpu=timestamp,name,utilization.gpu,memory.used,memory.total,temperature.gpu --format=csv >> /data_nvme/react_training/logs/gpu_monitor.log
    sleep 60
done &
```

#### 3. ä½¿ç”¨ TensorBoard å¯è§†åŒ–

```bash
# åœ¨æœåŠ¡å™¨ä¸Šå¯åŠ¨ TensorBoard
source /data_nvme/react_training/react_env/bin/activate
tensorboard --logdir=/data_nvme/react_training/logs --host=0.0.0.0 --port=6006

# é€šè¿‡ SSH éš§é“è®¿é—®ï¼ˆåœ¨æœ¬åœ°ç”µè„‘è¿è¡Œï¼‰
ssh -L 6006:localhost:6006 username@server-ip

# ç„¶åŽåœ¨æœ¬åœ°æµè§ˆå™¨æ‰“å¼€
# http://localhost:6006
```

#### 4. æ£€æŸ¥è®­ç»ƒè¿›åº¦

```bash
# æŸ¥çœ‹è¾“å‡ºç›®å½•
ls -lh /data_nvme/react_training/output/qwen2.5-7b-react/

# æŸ¥çœ‹æ£€æŸ¥ç‚¹
ls -lh /data_nvme/react_training/output/qwen2.5-7b-react/checkpoint-*/

# æŸ¥çœ‹è®­ç»ƒæ›²çº¿ï¼ˆå¦‚æžœå¯ç”¨äº† plot_lossï¼‰
ls /data_nvme/react_training/output/qwen2.5-7b-react/training_loss.png
```

---

### é˜¶æ®µ 7: è®­ç»ƒå®ŒæˆåŽçš„å¤„ç†

#### 1. è¯„ä¼°æ¨¡åž‹

```bash
# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
cd /data_nvme/react_training/LLaMA-Factory

llamafactory-cli train \
    --stage sft \
    --do_eval \
    --model_name_or_path Qwen/Qwen2.5-7B-Instruct \
    --adapter_name_or_path /data_nvme/react_training/output/qwen2.5-7b-react \
    --dataset react_testing \
    --template qwen \
    --finetuning_type lora \
    --output_dir /data_nvme/react_training/evaluation \
    --per_device_eval_batch_size 4 \
    --predict_with_generate \
    --bf16
```

#### 2. åˆå¹¶ LoRA æƒé‡

```bash
# å°† LoRA åˆå¹¶åˆ°åŸºç¡€æ¨¡åž‹
llamafactory-cli export \
    --model_name_or_path Qwen/Qwen2.5-7B-Instruct \
    --adapter_name_or_path /data_nvme/react_training/output/qwen2.5-7b-react \
    --template qwen \
    --finetuning_type lora \
    --export_dir /data_nvme/react_training/merged_model \
    --export_size 2 \
    --export_device cpu \
    --export_legacy_format False
```

#### 3. ä¸‹è½½æ¨¡åž‹åˆ°æœ¬åœ°

```bash
# åœ¨æœ¬åœ°ç”µè„‘è¿è¡Œ

# ä¸‹è½½åˆå¹¶åŽçš„æ¨¡åž‹
scp -r username@server-ip:/data_nvme/react_training/merged_model ./

# æˆ–ä¸‹è½½ LoRA é€‚é…å™¨ï¼ˆæ›´å°ï¼‰
scp -r username@server-ip:/data_nvme/react_training/output/qwen2.5-7b-react ./

# ä½¿ç”¨ rsyncï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
rsync -avz --progress username@server-ip:/data_nvme/react_training/merged_model ./
```

---

## ðŸš€ å¿«é€Ÿå¯åŠ¨è„šæœ¬

åˆ›å»ºä¸€é”®éƒ¨ç½²è„šæœ¬ï¼š

```bash
cat > /data_nvme/react_training/quick_start.sh << 'EOF'
#!/bin/bash
set -e

echo "ðŸš€ ReAct æ¨¡åž‹è®­ç»ƒ - å¿«é€Ÿå¯åŠ¨"
echo "================================"

# 1. åˆ›å»ºç›®å½•
echo "ðŸ“ åˆ›å»ºç›®å½•ç»“æž„..."
mkdir -p /data_nvme/react_training/{data,models,output,logs,scripts}

# 2. åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒ
echo "ðŸ é…ç½® Python çŽ¯å¢ƒ..."
cd /data_nvme/react_training
python -m venv react_env
source react_env/bin/activate

# 3. å®‰è£…ä¾èµ–
echo "ðŸ“¦ å®‰è£…ä¾èµ–ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰..."
pip install --upgrade pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# 4. å®‰è£… LLaMA-Factory
echo "ðŸ”§ å®‰è£… LLaMA-Factory..."
if [ ! -d "LLaMA-Factory" ]; then
    git clone https://github.com/hiyouga/LLaMA-Factory.git
fi
cd LLaMA-Factory
pip install -e .[torch,metrics]

# 5. éªŒè¯
echo "âœ… éªŒè¯å®‰è£…..."
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"

echo ""
echo "âœ¨ çŽ¯å¢ƒé…ç½®å®Œæˆï¼"
echo ""
echo "ä¸‹ä¸€æ­¥ï¼š"
echo "1. ä¸Šä¼ æ•°æ®åˆ° /data_nvme/react_training/data/"
echo "2. é…ç½® dataset_info.json"
echo "3. è¿è¡Œè®­ç»ƒ: ./scripts/train.sh"
EOF

chmod +x /data_nvme/react_training/quick_start.sh
```

---

## ðŸ“Š é¢„æœŸæ€§èƒ½

åŸºäºŽ RTX 5090 çš„é…ç½®ï¼š

| æ¨¡åž‹ | Batch Size | è®­ç»ƒæ—¶é—´ | æ˜¾å­˜å ç”¨ | æŽ¨è |
|------|-----------|---------|---------|------|
| Qwen2.5-3B | 8 | ~1.5h | ~12GB | â­â­â­ |
| Qwen2.5-7B | 4 | ~3h | ~18GB | â­â­â­â­â­ |
| Qwen2.5-14B | 2 | ~6h | ~22GB | â­â­â­â­ |
| Qwen2.5-32B | 1 (QLoRA) | ~15h | ~23GB | â­â­â­ |

**æŽ¨è**: Qwen2.5-7Bï¼Œæ€§èƒ½ä¸Žé€Ÿåº¦æœ€å¹³è¡¡

---

## ðŸ’¡ ä¼˜åŒ–å»ºè®®

### æå‡è®­ç»ƒé€Ÿåº¦

1. **ä½¿ç”¨ Flash Attention 2**
```bash
pip install flash-attn --no-build-isolation
# åœ¨é…ç½®ä¸­æ·»åŠ  use_flash_attention_2: true
```

2. **å¢žå¤§ Batch Size**
```yaml
per_device_train_batch_size: 6  # RTX 5090 å¯ä»¥æ”¯æŒ
gradient_accumulation_steps: 2
```

3. **ä½¿ç”¨ BF16**ï¼ˆRTX 5090 æ”¯æŒï¼‰
```yaml
bf16: true
fp16: false
```

4. **ä½¿ç”¨ Fused Optimizer**
```yaml
optim: adamw_torch_fused
```

### èŠ‚çœæ˜¾å­˜

1. **ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹**
```yaml
gradient_checkpointing: true
```

2. **å‡å°åºåˆ—é•¿åº¦**ï¼ˆå¦‚æžœå¯èƒ½ï¼‰
```yaml
cutoff_len: 1536  # ä»Ž 2048 å‡å°
```

3. **ä½¿ç”¨ QLoRA**
```yaml
quantization_bit: 4
```

---

## â— å¸¸è§é—®é¢˜

### Q1: è¿žæŽ¥æ–­å¼€åŽè®­ç»ƒä¼šåœæ­¢å—ï¼Ÿ

**A**: å¦‚æžœä½¿ç”¨ `nohup` æˆ– `tmux`ï¼Œä¸ä¼šåœæ­¢ã€‚æŽ¨èä½¿ç”¨ tmuxã€‚

### Q2: å¦‚ä½•æš‚åœå’Œæ¢å¤è®­ç»ƒï¼Ÿ

**A**: LLaMA-Factory æ”¯æŒæ–­ç‚¹ç»­è®­ï¼š
```bash
# ä»Žæ£€æŸ¥ç‚¹æ¢å¤
--resume_from_checkpoint /data_nvme/react_training/output/qwen2.5-7b-react/checkpoint-XXX
```

### Q3: æ˜¾å­˜æº¢å‡ºæ€Žä¹ˆåŠžï¼Ÿ

**A**: å‡å° batch size æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼š
```yaml
per_device_train_batch_size: 2  # å‡å°
gradient_accumulation_steps: 8  # å¢žå¤§
```

### Q4: è®­ç»ƒå¤ªæ…¢ï¼Ÿ

**A**: 
1. æ£€æŸ¥æ˜¯å¦å®‰è£… Flash Attention
2. ä½¿ç”¨ BF16
3. å¢žå¤§ batch size
4. ç¡®ä¿ä½¿ç”¨ NVMe å­˜å‚¨

---

## ðŸŽ¯ å®Œæ•´å‘½ä»¤é€ŸæŸ¥è¡¨

```bash
# è¿žæŽ¥æœåŠ¡å™¨
ssh username@server-ip

# æ¿€æ´»çŽ¯å¢ƒ
source /data_nvme/react_training/react_env/bin/activate

# æŸ¥çœ‹ GPU
nvidia-smi

# å¼€å§‹è®­ç»ƒï¼ˆtmuxï¼‰
tmux new -s react_training
./scripts/train.sh

# åˆ†ç¦» tmux
Ctrl+B, D

# é‡æ–°è¿žæŽ¥
tmux attach -t react_training

# æŸ¥çœ‹æ—¥å¿—
tail -f /data_nvme/react_training/logs/train.log

# ç›‘æŽ§ GPU
watch -n 1 nvidia-smi

# æŸ¥çœ‹è®­ç»ƒè¿›åº¦
ls -lh /data_nvme/react_training/output/qwen2.5-7b-react/

# å¯åŠ¨ TensorBoard
tensorboard --logdir=/data_nvme/react_training/logs --host=0.0.0.0 --port=6006

# ä¸‹è½½æ¨¡åž‹
rsync -avz --progress username@server-ip:/data_nvme/react_training/merged_model ./
```

---

ç¥è®­ç»ƒé¡ºåˆ©ï¼ðŸŽ‰

